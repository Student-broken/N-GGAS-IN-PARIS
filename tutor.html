<script type="module">
    // --- THIS IS THE NEW, ROBUST LOADING SCRIPT ---
    import { pipeline, AutoTokenizer, AutoModelForSeq2SeqLM } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1';
    
    const status = document.getElementById('status');
    const chatContainer = document.getElementById('chat-container');
    const userInput = document.getElementById('user-input');
    const sendBtn = document.getElementById('send-btn');
    const aiModeCheckbox = document.getElementById('ai-mode-checkbox');
    const debugLog = document.getElementById('debug-log');
    const debugHeader = document.getElementById('debug-header');

    let tutorPipeline = null;
    let studentDataContext = '';
    let currentAiMode = 'tuteur';

    const PROMPT_TUTEUR = `Tu es un tuteur académique expert, amical et encourageant...`; // (content is the same as before)
    const PROMPT_ANALYSTE = `Tu es un moteur d'analyse de données purement logique et objectif...`; // (content is the same as before)

    function logToDebug(message) {
        const timestamp = new Date().toLocaleTimeString();
        debugLog.innerHTML += `[${timestamp}] ${message}\n`;
        debugLog.scrollTop = debugLog.scrollHeight;
    }

    function addMessage(text, sender) {
        const messageDiv = document.createElement('div');
        messageDiv.classList.add('message', `${sender}-message`);
        messageDiv.textContent = text;
        chatContainer.appendChild(messageDiv);
        chatContainer.scrollTop = chatContainer.scrollHeight;
        return messageDiv;
    }
    
    function formatDataForSLM(rawData, analysisData) {
        logToDebug("Formatting data into text context...");
        let summary = `DONNÉES BRUTES DE L'ÉTUDIANT:\n`;
        if (rawData?.nom) summary += `Nom: ${rawData.nom}\n`;
        ['etape1', 'etape2'].forEach(etape => {
            if (rawData[etape] && rawData[etape].length > 0) {
                summary += `\n--- RÉSULTATS ${etape.toUpperCase()} ---\n`;
                rawData[etape].forEach(subject => {
                    summary += `Sujet: ${subject.name} (${subject.code})\n`;
                    subject.competencies.forEach(comp => {
                        comp.assignments.forEach(assign => {
                            if (assign.result) summary += `- Travail: "${assign.work}", Résultat: ${assign.result}\n`;
                        });
                    });
                });
            }
        });
        if (analysisData) {
            summary += `\n\nANALYSE PRÉDICTIVE ET STATISTIQUES:\n`;
            if(analysisData.globalAverage) summary += `- Moyenne générale (connue): ${analysisData.globalAverage.toFixed(2)}%\n`;
            if(analysisData.burnoutRiskScore) summary += `- Score de risque de burnout: ${analysisData.burnoutRiskScore.toFixed(0)}/100\n`;
            if(analysisData.globalConsistencyScore) summary += `- Indice de consistance globale: ${analysisData.globalConsistencyScore.toFixed(0)}/100\n`;
            if(analysisData.predictions?.global?.p50) summary += `- Prédiction de la moyenne finale (Médiane P50): ${analysisData.predictions.global.p50.toFixed(2)}%\n`;
        }
        logToDebug(`Context formatting complete. Length: ${summary.length}`);
        return summary;
    }

    async function handleSend() {
        const query = userInput.value.trim();
        if (query === '' || !tutorPipeline) return;
        addMessage(query, 'user');
        userInput.value = '';
        
        const loadingMessage = addMessage("Réflexion en cours...", 'tutor');
        loadingMessage.classList.add('loading');
        
        const systemPrompt = currentAiMode === 'tuteur' ? PROMPT_TUTEUR : PROMPT_ANALYSTE;
        const fullPrompt = `${systemPrompt}\n\n--- DONNÉES DE L'ÉTUDIANT ---\n${studentDataContext}\n--- FIN DES DONNÉES ---\nQuestion: ${query}`;
        
        try {
            const result = await tutorPipeline(fullPrompt, {
                max_new_tokens: 300,
                do_sample: true,
                temperature: 0.7,
                top_k: 50,
            });
            loadingMessage.textContent = result[0].generated_text;
            loadingMessage.classList.remove('loading');
        } catch (e) {
            loadingMessage.textContent = "Désolé, une erreur est survenue. Veuillez réessayer.";
            console.error("Inference error:", e);
        }
    }

    async function main() {
        logToDebug("Initializing Tutor AI (V3 - Manual Assembly)...");
        try {
            const rawData = JSON.parse(localStorage.getItem('mbsData'));
            const analysisData = JSON.parse(localStorage.getItem('mbsProjectionCache'));
            if (!rawData?.valid) {
                status.textContent = "Données 'mbsData' introuvables.";
                return;
            }
            studentDataContext = formatDataForSLM(rawData, analysisData);

            const model_id = 'Xenova/LaMini-Flan-T5-77M';
            const task = 'text2text-generation';
            
            // --- MANUAL ASSEMBLY PROCESS ---
            logToDebug(`STEP 1: Loading Tokenizer for model [${model_id}]`);
            status.textContent = "Étape 1/3 : Chargement du tokenizer...";
            const tokenizer = await AutoTokenizer.from_pretrained(model_id);
            logToDebug("Tokenizer loaded successfully.");

            logToDebug(`STEP 2: Loading Main Model for model [${model_id}]`);
            status.textContent = "Étape 2/3 : Chargement du modèle principal (peut être long)...";
            const model = await AutoModelForSeq2SeqLM.from_pretrained(model_id, {
                progress_callback: (p) => {
                    status.textContent = `Étape 2/3 : Chargement... (${p.file.split('/').pop()}: ${Math.round(p.progress)}%)`;
                }
            });
            logToDebug("Main model loaded successfully.");

            logToDebug("STEP 3: Assembling pipeline...");
            status.textContent = "Étape 3/3 : Assemblage du pipeline...";
            tutorPipeline = await pipeline(task, null, { // Pass null for model name
                model: model,       // Provide the pre-loaded model
                tokenizer: tokenizer // Provide the pre-loaded tokenizer
            });
            logToDebug("Pipeline assembled successfully! Tutor is ready.");
            
            status.textContent = `Bonjour, ${rawData.nom}! Je suis prêt.`;
            userInput.disabled = false;
            sendBtn.disabled = false;
            addMessage("J'ai analysé vos données académiques. Comment puis-je vous aider aujourd'hui ?", 'tutor');

        } catch (e) {
            status.textContent = "Une erreur critique est survenue. Veuillez rafraîchir la page.";
            logToDebug(`CRITICAL ERROR in main(): ${e.message}\n${e.stack}`);
            console.error(e);
        }
    }

    sendBtn.addEventListener('click', handleSend);
    userInput.addEventListener('keypress', (e) => { if (e.key === 'Enter') handleSend(); });
    aiModeCheckbox.addEventListener('change', (e) => {
        currentAiMode = e.target.checked ? 'analyste' : 'tuteur';
        logToDebug(`AI Mode changed -> ${currentAiMode.toUpperCase()}`);
    });
    debugHeader.addEventListener('click', () => document.getElementById('debug-panel').classList.toggle('visible'));

    main();
</script>
